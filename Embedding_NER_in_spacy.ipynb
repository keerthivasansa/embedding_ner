{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     --------------------------------------- 0.1/12.8 MB 656.4 kB/s eta 0:00:20\n",
            "      --------------------------------------- 0.2/12.8 MB 1.3 MB/s eta 0:00:10\n",
            "     - -------------------------------------- 0.6/12.8 MB 3.3 MB/s eta 0:00:04\n",
            "     --- ------------------------------------ 1.1/12.8 MB 4.5 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.5/12.8 MB 5.8 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 1.9/12.8 MB 6.2 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.3/12.8 MB 6.3 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 2.9/12.8 MB 7.3 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.5/12.8 MB 8.1 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.5/12.8 MB 9.0 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 5.3/12.8 MB 9.6 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 5.9/12.8 MB 9.9 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 6.5/12.8 MB 9.8 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 10.6 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.5/12.8 MB 11.8 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.4/12.8 MB 12.3 MB/s eta 0:00:01\n",
            "     ------------------------------- ------- 10.2/12.8 MB 12.6 MB/s eta 0:00:01\n",
            "     --------------------------------- ----- 10.9/12.8 MB 15.2 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 11.4/12.8 MB 15.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 12.1/12.8 MB 16.4 MB/s eta 0:00:01\n",
            "     --------------------------------------  12.8/12.8 MB 16.8 MB/s eta 0:00:01\n",
            "     --------------------------------------- 12.8/12.8 MB 15.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in .\\venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in .\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in .\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in .\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in .\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in .\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in .\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in .\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in .\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in .\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in .\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in .\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in .\\venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in .\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in .\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in .\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in .\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "pFSYhLbYeLBz"
      },
      "outputs": [],
      "source": [
        "# NER Annotator: https://tecoholic.github.io/ner-annotator/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vc7cYR16eL79"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "import json\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc_bin = DocBin()\n",
        "\n",
        "with open(\"data/annotations_med.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "classes = data['classes']\n",
        "annotations = data['annotations']\n",
        "\n",
        "for annotation in annotations:\n",
        "    text = annotation[0]\n",
        "    entities = annotation[1][\"entities\"]\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    ents = []\n",
        "    for start, end, label in entities:\n",
        "        ents.append((start, end, label))\n",
        "\n",
        "    doc.ents = [doc.char_span(start, end, label=label) for start, end, label in ents]\n",
        "\n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"model/training_data.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOdqdKqShW4Q",
        "outputId": "83ea2b50-125b-42b6-ecc1-a3e323cc27e8"
      },
      "outputs": [],
      "source": [
        "classes = ('PROCEDURE / TEST', 'DRUG', 'CONDITION', 'SYMPTOM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds4dw76Bgomp",
        "outputId": "acc24d6e-6fb4-4d93-9b10-4f8889256b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Losses: {'ner': 962.2629118859768}\n",
            "Epoch 2, Losses: {'ner': 142.46544225673108}\n",
            "Epoch 3, Losses: {'ner': 108.40111186464276}\n",
            "Epoch 4, Losses: {'ner': 103.71139129452962}\n",
            "Epoch 5, Losses: {'ner': 102.44336059241577}\n",
            "Epoch 6, Losses: {'ner': 93.48110673990135}\n",
            "Epoch 7, Losses: {'ner': 90.24217953867446}\n",
            "Epoch 8, Losses: {'ner': 78.65201494982807}\n",
            "Epoch 9, Losses: {'ner': 78.7146151953392}\n",
            "Epoch 10, Losses: {'ner': 70.98430156292133}\n",
            "Epoch 11, Losses: {'ner': 72.75402945383172}\n",
            "Epoch 12, Losses: {'ner': 68.91185505818275}\n",
            "Epoch 13, Losses: {'ner': 61.294393109088034}\n",
            "Epoch 14, Losses: {'ner': 66.53415800741051}\n",
            "Epoch 15, Losses: {'ner': 64.08468256028532}\n",
            "Epoch 16, Losses: {'ner': 51.83209046303569}\n",
            "Epoch 17, Losses: {'ner': 56.27322734381741}\n",
            "Epoch 18, Losses: {'ner': 53.83540639402655}\n",
            "Epoch 19, Losses: {'ner': 60.29831454971595}\n",
            "Epoch 20, Losses: {'ner': 56.80565134748797}\n",
            "Epoch 21, Losses: {'ner': 47.55866545214897}\n",
            "Epoch 22, Losses: {'ner': 44.33317708948885}\n",
            "Epoch 23, Losses: {'ner': 39.10867803560965}\n",
            "Epoch 24, Losses: {'ner': 45.769692546273106}\n",
            "Epoch 25, Losses: {'ner': 36.69704645190786}\n",
            "Epoch 26, Losses: {'ner': 41.69019370487287}\n",
            "Epoch 27, Losses: {'ner': 45.32275915598954}\n",
            "Epoch 28, Losses: {'ner': 38.54377701476286}\n",
            "Epoch 29, Losses: {'ner': 38.96030073982907}\n",
            "Epoch 30, Losses: {'ner': 38.284786985282956}\n",
            "Epoch 31, Losses: {'ner': 31.727614585232626}\n",
            "Epoch 32, Losses: {'ner': 35.4894455090818}\n",
            "Epoch 33, Losses: {'ner': 38.37910144139845}\n",
            "Epoch 34, Losses: {'ner': 36.47317263990517}\n",
            "Epoch 35, Losses: {'ner': 28.196823931923387}\n",
            "Epoch 36, Losses: {'ner': 31.010061681022062}\n",
            "Epoch 37, Losses: {'ner': 22.947302689240395}\n",
            "Epoch 38, Losses: {'ner': 23.696708376301274}\n",
            "Epoch 39, Losses: {'ner': 25.522918805583544}\n",
            "Epoch 40, Losses: {'ner': 23.22942960130903}\n",
            "Epoch 41, Losses: {'ner': 25.55279066945614}\n",
            "Epoch 42, Losses: {'ner': 21.672342253555023}\n",
            "Epoch 43, Losses: {'ner': 26.953844184817985}\n",
            "Epoch 44, Losses: {'ner': 23.92347177016197}\n",
            "Epoch 45, Losses: {'ner': 28.817897126394133}\n",
            "Epoch 46, Losses: {'ner': 19.777864401868413}\n",
            "Epoch 47, Losses: {'ner': 20.494508240518215}\n",
            "Epoch 48, Losses: {'ner': 21.506289017553467}\n",
            "Epoch 49, Losses: {'ner': 25.11983736783368}\n",
            "Epoch 50, Losses: {'ner': 17.718057059155512}\n",
            "Epoch 51, Losses: {'ner': 18.485598818226457}\n",
            "Epoch 52, Losses: {'ner': 25.673645592977266}\n",
            "Epoch 53, Losses: {'ner': 12.499390746624996}\n",
            "Epoch 54, Losses: {'ner': 26.299060231956542}\n",
            "Epoch 55, Losses: {'ner': 13.666473239578472}\n",
            "Epoch 56, Losses: {'ner': 13.44765492682181}\n",
            "Epoch 57, Losses: {'ner': 11.175660858091623}\n",
            "Epoch 58, Losses: {'ner': 20.447517596512647}\n",
            "Epoch 59, Losses: {'ner': 18.804774610656636}\n",
            "Epoch 60, Losses: {'ner': 15.302329526621781}\n",
            "Epoch 61, Losses: {'ner': 8.519916362618277}\n",
            "Epoch 62, Losses: {'ner': 17.14247891156696}\n",
            "Epoch 63, Losses: {'ner': 17.584731310555284}\n",
            "Epoch 64, Losses: {'ner': 15.184572810447536}\n",
            "Epoch 65, Losses: {'ner': 13.553999297174443}\n",
            "Epoch 66, Losses: {'ner': 10.864164921081315}\n",
            "Epoch 67, Losses: {'ner': 6.913751118467321}\n",
            "Epoch 68, Losses: {'ner': 9.691402998086145}\n",
            "Epoch 69, Losses: {'ner': 18.97239585152791}\n",
            "Epoch 70, Losses: {'ner': 13.313172467242623}\n",
            "Epoch 71, Losses: {'ner': 16.039992147546407}\n",
            "Epoch 72, Losses: {'ner': 15.623994859301794}\n",
            "Epoch 73, Losses: {'ner': 11.569604187465426}\n",
            "Epoch 74, Losses: {'ner': 9.916844951047324}\n",
            "Epoch 75, Losses: {'ner': 17.264424455293465}\n",
            "Epoch 76, Losses: {'ner': 7.040640825011307}\n",
            "Epoch 77, Losses: {'ner': 6.447041128256388}\n",
            "Epoch 78, Losses: {'ner': 9.41287504456485}\n",
            "Epoch 79, Losses: {'ner': 7.332019983189334}\n",
            "Epoch 80, Losses: {'ner': 5.818734099503857}\n",
            "Epoch 81, Losses: {'ner': 10.611113307614483}\n",
            "Epoch 82, Losses: {'ner': 5.266072949451091}\n",
            "Epoch 83, Losses: {'ner': 16.889345975633145}\n",
            "Epoch 84, Losses: {'ner': 7.629873120203021}\n",
            "Epoch 85, Losses: {'ner': 6.9638018162646125}\n",
            "Epoch 86, Losses: {'ner': 8.266688638474905}\n",
            "Epoch 87, Losses: {'ner': 7.222689865165114}\n",
            "Epoch 88, Losses: {'ner': 11.560550998741553}\n",
            "Epoch 89, Losses: {'ner': 10.933965782680271}\n",
            "Epoch 90, Losses: {'ner': 8.977667525612603}\n",
            "Epoch 91, Losses: {'ner': 9.130547872329359}\n",
            "Epoch 92, Losses: {'ner': 10.378715296217662}\n",
            "Epoch 93, Losses: {'ner': 5.984935762605453}\n",
            "Epoch 94, Losses: {'ner': 13.802360198828536}\n",
            "Epoch 95, Losses: {'ner': 9.13183586121933}\n",
            "Epoch 96, Losses: {'ner': 5.812265945114483}\n",
            "Epoch 97, Losses: {'ner': 12.008315155256183}\n",
            "Epoch 98, Losses: {'ner': 14.185334302285769}\n",
            "Epoch 99, Losses: {'ner': 4.81685372596293}\n",
            "Epoch 100, Losses: {'ner': 9.129252627601687}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch\n",
        "import random\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "for label in classes:\n",
        "    ner.add_label(label)\n",
        "\n",
        "doc_bin = DocBin().from_disk(\"model/training_data.spacy\")\n",
        "docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "for epoch in range(100):\n",
        "    losses = {}\n",
        "    random.shuffle(docs)\n",
        "    for batch in minibatch(docs, size=8):\n",
        "        for doc in batch:\n",
        "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Epoch {epoch + 1}, Losses: {losses}\")\n",
        "\n",
        "nlp.to_disk(\"model/trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0CS1c7knn5w6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QaIzayV3oxy4"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jUzlWoFr3Fgs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvJaTbrtjDJf",
        "outputId": "20fcb340-05d8-4a16-ef3c-866136d37385"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Keerthi\n",
            "[nltk_data]     Vasan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoTw90De5MW2",
        "outputId": "8dc21cbf-8d4b-4139-f921-e6b6cf15b583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Keerthi\n",
            "[nltk_data]     Vasan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SLsEjZIZ5Iou"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jJuRPjkZpsO7"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  return np.dot(v1, v2) / (norm(v1) * norm(v2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "\n",
        "\n",
        "class TextCorpusSearcher:\n",
        "    def __init__(self, filename, x, label):\n",
        "        self.label = label\n",
        "\n",
        "        nlp = spacy.load('en_core_web_sm')\n",
        "        doc = ' '.join(x).lower()\n",
        "\n",
        "        # self.stemmer = SnowballStemmer(language='english')\n",
        "        self.x = [word.text for word in nlp(doc) if not (word.is_space or word.is_punct)]\n",
        "        text = self.get_text(filename)\n",
        "        sentences = []\n",
        "        \n",
        "        for sent in sent_tokenize(text.lower()):\n",
        "            s = []\n",
        "            for word in nlp(sent):\n",
        "                if word.is_punct or word.is_space:\n",
        "                    continue\n",
        "                s.append(word.text)\n",
        "            sentences.append(s)\n",
        "\n",
        "        self.model = Word2Vec(sentences, vector_size=100, window=8, min_count=1, sg=1)\n",
        "        self.model.train(sentences, total_examples=self.model.corpus_count, epochs=100)\n",
        "\n",
        "        for w in self.x:\n",
        "            if w not in self.model.wv:\n",
        "                print(\"[WARN]\", w, \"missing in Word2Vec training data\")\n",
        "\n",
        "    def get_text(self, filename):\n",
        "        with open(filename) as f:\n",
        "            return f.read() \n",
        "    \n",
        "    def get_embed(self, word):\n",
        "        return self.model.wv[word.lower()]\n",
        "    \n",
        "    def has_embed(self, word):\n",
        "        return word in self.model.wv\n",
        "\n",
        "    def get_score(self, word):\n",
        "        global curr_model\n",
        "        word = word.lower()\n",
        "        tag = pos_tag([word])[0][1]\n",
        "        empty = np.zeros(len(self.x))\n",
        "\n",
        "        if not tag.startswith('NN') or word not in self.model.wv:\n",
        "            return empty\n",
        "        \n",
        "        curr_model = self.model\n",
        "        scores = []\n",
        "        for w in self.x:\n",
        "            if w not in self.model.wv:\n",
        "                continue\n",
        "            score = cosine_similarity(self.model.wv[word], self.model.wv[w])\n",
        "            scores.append(score)\n",
        "        \n",
        "        return scores    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "def create_emb_ner(config_file):\n",
        "    f = open(config_file, 'rb')\n",
        "    config = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "    searchers = []\n",
        "\n",
        "    for e in config:\n",
        "        labels = e['labels']\n",
        "        name = e['name']\n",
        "\n",
        "        max_thresh = e.get('max_thresh', 0.5)\n",
        "        min_thresh = e.get('min_thresh', 0.2)\n",
        "        \n",
        "        split_ratio = 0.85\n",
        "\n",
        "        split = int(len(labels) * split_ratio)\n",
        "\n",
        "        train_terms = labels[:split]\n",
        "        \n",
        "        searcher = TextCorpusSearcher(\n",
        "            'data/train-medical.txt',\n",
        "            train_terms,\n",
        "            name\n",
        "        )\n",
        "\n",
        "        searchers.append((searcher, min_thresh, max_thresh, name))\n",
        "\n",
        "    tagger = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def custom_ner_component(doc):\n",
        "        new_entities = [ent for ent in doc.ents]\n",
        "        for index, token in enumerate(doc):\n",
        "            tag = tagger(token.text)[0].pos_\n",
        "            if token.ent_type != 0 or tag != 'NOUN':\n",
        "                continue\n",
        "            maxLabel = \"\"\n",
        "            \n",
        "            maxScore = 0\n",
        "            maxLabel = ''\n",
        "\n",
        "            print('Searching for', token)\n",
        "            for searcher, min_thresh, max_thresh, label in searchers:\n",
        "                scores = searcher.get_score(token.text)\n",
        "                currMax = max(scores)\n",
        "\n",
        "                if min(scores) < min_thresh or currMax < max_thresh:\n",
        "                    continue\n",
        "                \n",
        "                print('Passed for', label)\n",
        "                print(scores)\n",
        "\n",
        "                s = mean([currMax, min(scores)])\n",
        "                \n",
        "                if s > maxScore:\n",
        "                    maxLabel = label\n",
        "                    maxScore = s\n",
        "            \n",
        "            if maxScore > 0:\n",
        "                print(\"Adding tag for \", token, maxLabel, \"with score\", maxScore)\n",
        "                new_entities.append(Span(doc, index, index + 1, label=maxLabel))\n",
        "        \n",
        "        doc.ents = new_entities\n",
        "        return doc\n",
        "    \n",
        "    return custom_ner_component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[E004] Can't set up pipeline component: a factory for 'embed_ner2' already exists. Existing factory: <function create_embedding_component at 0x000001E0123620C0>. New factory: <function create_embedding_component at 0x000001E0134D7F60>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[400], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;129;43m@Language\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membed_ner2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mcreate_embedding_component\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_emb_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\embedding_ner\\venv\\Lib\\site-packages\\spacy\\language.py:518\u001b[0m, in \u001b[0;36mLanguage.factory.<locals>.add_factory\u001b[1;34m(factory_func)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_same_func(factory_func, existing_func):\n\u001b[0;32m    515\u001b[0m         err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE004\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    516\u001b[0m             name\u001b[38;5;241m=\u001b[39mname, func\u001b[38;5;241m=\u001b[39mexisting_func, new_func\u001b[38;5;241m=\u001b[39mfactory_func\n\u001b[0;32m    517\u001b[0m         )\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    520\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_arg_names(factory_func)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_names \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_names:\n",
            "\u001b[1;31mValueError\u001b[0m: [E004] Can't set up pipeline component: a factory for 'embed_ner2' already exists. Existing factory: <function create_embedding_component at 0x000001E0123620C0>. New factory: <function create_embedding_component at 0x000001E0134D7F60>"
          ]
        }
      ],
      "source": [
        "from spacy import Language\n",
        "\n",
        "@Language.factory(name='embed_ner2', default_config={})\n",
        "def create_embedding_component(nlp, name, config_file):\n",
        "    return create_emb_ner(config_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "FaP0QVrehdcr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = open(\"data/train-medical.txt\")\n",
        "tokens = word_tokenize(t.read())\n",
        "t.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOUN\n"
          ]
        }
      ],
      "source": [
        "t = spacy.load('en_core_web_sm')\n",
        "a = t('pain')[0]\n",
        "print(a.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_entity(entity, max_thresh, min_thresh):\n",
        "    labels = entity['labels']\n",
        "    name = entity['name']\n",
        "\n",
        "    split_ratio = 0.85\n",
        "\n",
        "    split = int(len(labels) * split_ratio)\n",
        "\n",
        "    train_terms = labels[:split]\n",
        "    test_terms = labels[split:]\n",
        "    \n",
        "    searcher = TextCorpusSearcher(\n",
        "        'data/train-medical.txt',\n",
        "        train_terms,\n",
        "        name\n",
        "    )\n",
        "    \n",
        "    x_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for w in test_terms:\n",
        "        x_test.append(w)\n",
        "        y_test.append(1)\n",
        "    \n",
        "    while len(y_test) < len(test_terms) * 2:\n",
        "        w = random.choice(tokens)\n",
        "        if w in stop_words or w in ',.:;' or w in labels:\n",
        "            continue\n",
        "        x_test.append(w)\n",
        "        y_test.append(0)\n",
        "\n",
        "    pred = []\n",
        "    for w in x_test:\n",
        "        scores = searcher.get_score(w)\n",
        "        min_score = min(scores)\n",
        "        max_score = max(scores)\n",
        "        p = 1 if (min_score >= min_thresh and max_score >= max_thresh) else 0\n",
        "        pred.append(p)\n",
        "\n",
        "    return y_test, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'Condition',\n",
              " 'labels': ['gallstones',\n",
              "  'acute pancreatitis',\n",
              "  'hepatic steatosis',\n",
              "  'pancreatitis',\n",
              "  'tachycardia',\n",
              "  'palpitations',\n",
              "  'arrhythmias',\n",
              "  'myocardial infarction',\n",
              "  'hypertension',\n",
              "  'diabetes',\n",
              "  'cardiomyopathy',\n",
              "  'stroke']}"
            ]
          },
          "execution_count": 500,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "for i in range(4):\n",
        "    y_test, pred = test_entity(config[i], min_thresh=0.15, max_thresh=0.6)\n",
        "    f1_scores.append(f1_score(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.725"
            ]
          },
          "execution_count": 533,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(f1_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 503,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Without Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"model/trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_string = \"I had chest pain and rash in my left leg, so I went to see the doctor. He ran a biopsy and some scans. Unfortunately, I got diagnozed with pancreatitis and he said I had a high chance of getting stroke. The doctor finally prescribed me aspirin and prednisone.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = nlp(test_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I had \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chest pain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMPTOM</span>\n",
              "</mark>\n",
              " and rash in my left leg, so I went to see the doctor. He ran a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    biopsy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROCEDURE / TESTS</span>\n",
              "</mark>\n",
              " and some scans. Unfortunately, I got diagnozed with \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    pancreatitis\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CONDITION</span>\n",
              "</mark>\n",
              " and he said I had a high chance of getting stroke. The doctor finally prescribed me \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    aspirin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DRUG</span>\n",
              "</mark>\n",
              " and prednisone.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding NER test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.create_emb_ner.<locals>.custom_ner_component(doc)>"
            ]
          },
          "execution_count": 539,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.add_pipe('embed_ner2', config={\n",
        "    'config_file': 'ner_config.json'\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With Embedding NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for rash\n",
            "Passed for Condition\n",
            "[0.33655444, 0.40822273, 0.21276493, 0.3220555, 0.33729333, 0.21276493, 0.5105303, 0.49390528, 0.39802164, 0.46124947, 0.42895567, 0.3183581, 0.3007358]\n",
            "Passed for Symptoms\n",
            "[0.31940433, 0.45735604, 0.4099632, 0.37925756, 0.49390528, 0.51933193, 0.5020989, 0.26578057, 0.38611507, 0.8166873, 0.87939787, 0.99999994]\n",
            "Adding tag for  rash Symptoms with score 0.6328902\n",
            "Searching for leg\n",
            "Searching for doctor\n",
            "Searching for scans\n",
            "Searching for chance\n",
            "Searching for stroke\n",
            "Passed for Drug\n",
            "[0.36774987, 0.39049277, 0.35076165, 0.6310034, 0.4010282, 0.42588294, 0.3669644]\n",
            "Passed for Condition\n",
            "[0.24415085, 0.8427904, 0.7777064, 0.6175021, 0.63150436, 0.7777064, 0.32434508, 0.33704415, 0.8486899, 0.969083, 0.95720136, 0.34242904, 0.41118282]\n",
            "Adding tag for  stroke Condition with score 0.6066169\n",
            "Searching for doctor\n",
            "Searching for prednisone\n",
            "Passed for Drug\n",
            "[0.46974435, 0.47893983, 0.99999994, 0.42986003, 0.6280245, 0.6434636, 0.44853935]\n",
            "Passed for Condition\n",
            "[0.604086, 0.4042642, 0.27584895, 0.2639454, 0.28180736, 0.27584895, 0.4413196, 0.39399293, 0.4209308, 0.40053806, 0.40600407, 0.4441037, 0.4474794]\n",
            "Passed for Symptoms\n",
            "[0.29500854, 0.38336712, 0.40366906, 0.48630735, 0.39399293, 0.40836626, 0.41568726, 0.36332315, 0.34608647, 0.7546246, 0.64827394, 0.5733621]\n",
            "Passed for Procedure\n",
            "[0.21481314, 0.37115574, 0.2188271, 0.40236056, 0.31450775, 0.23129992, 0.35706928, 0.21544875, 0.5246675, 0.23572947, 0.23984374, 0.323735]\n",
            "Adding tag for  prednisone Drug with score 0.71493\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(test_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I had \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chest pain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMPTOM</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    rash\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptoms</span>\n",
              "</mark>\n",
              " in my left leg, so I went to see the doctor. He ran a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    biopsy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROCEDURE / TESTS</span>\n",
              "</mark>\n",
              " and some scans. Unfortunately, I got diagnozed with \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    pancreatitis\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CONDITION</span>\n",
              "</mark>\n",
              " and he said I had a high chance of getting \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    stroke\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Condition</span>\n",
              "</mark>\n",
              ". The doctor finally prescribed me \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    aspirin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DRUG</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    prednisone\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
