{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 12.8/12.8 MB 3.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: colorama in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\project\\embedding_ner\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 24.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SkwkmQIiZO36"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_BIat3mqZoQA"
      },
      "outputs": [],
      "source": [
        "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q3p8v8SxZtxP"
      },
      "outputs": [],
      "source": [
        "text1= NER(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU8-1xL7ZqF9",
        "outputId": "f1d0797d-0f75-4f13-e5d2-c8b0514115e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Indian Space Research Organisation ORG\n",
            "India GPE\n",
            "Bengaluru GPE\n",
            "Department of Space ORG\n",
            "India GPE\n",
            "ISRO ORG\n",
            "DOS ORG\n"
          ]
        }
      ],
      "source": [
        "for word in text1.ents:\n",
        "    print(word.text,word.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pFSYhLbYeLBz"
      },
      "outputs": [],
      "source": [
        "# NER Annotator: https://tecoholic.github.io/ner-annotator/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vc7cYR16eL79"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc, DocBin\n",
        "import json\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc_bin = DocBin()\n",
        "\n",
        "with open(\"data/annotations.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "classes = data['classes']\n",
        "annotations = data['annotations']\n",
        "for annotation in annotations:\n",
        "    text = annotation[0]\n",
        "    entities = annotation[1][\"entities\"]\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    ents = []\n",
        "    for start, end, label in entities:\n",
        "        ents.append((start, end, label))\n",
        "\n",
        "    doc.ents = [doc.char_span(start, end, label=label) for start, end, label in ents]\n",
        "\n",
        "    doc_bin.add(doc)\n",
        "\n",
        "doc_bin.to_disk(\"model/training_data.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOdqdKqShW4Q",
        "outputId": "83ea2b50-125b-42b6-ecc1-a3e323cc27e8"
      },
      "outputs": [],
      "source": [
        "classes = ('PERSON', 'PLACE', 'SERVICE', 'TIME', 'DATE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds4dw76Bgomp",
        "outputId": "acc24d6e-6fb4-4d93-9b10-4f8889256b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Losses: {'ner': 64.92905551195145}\n",
            "Epoch 2, Losses: {'ner': 62.88421410322189}\n",
            "Epoch 3, Losses: {'ner': 57.87563592195511}\n",
            "Epoch 4, Losses: {'ner': 52.684956789016724}\n",
            "Epoch 5, Losses: {'ner': 41.17852871119976}\n",
            "Epoch 6, Losses: {'ner': 33.18318738695234}\n",
            "Epoch 7, Losses: {'ner': 29.434816773689818}\n",
            "Epoch 8, Losses: {'ner': 27.31542748703214}\n",
            "Epoch 9, Losses: {'ner': 29.216141655784668}\n",
            "Epoch 10, Losses: {'ner': 25.985767440293785}\n",
            "Epoch 11, Losses: {'ner': 28.69254120306141}\n",
            "Epoch 12, Losses: {'ner': 26.566457065397117}\n",
            "Epoch 13, Losses: {'ner': 23.863056193607918}\n",
            "Epoch 14, Losses: {'ner': 23.871349419852777}\n",
            "Epoch 15, Losses: {'ner': 21.741570208774647}\n",
            "Epoch 16, Losses: {'ner': 22.87053915472643}\n",
            "Epoch 17, Losses: {'ner': 20.01375056053439}\n",
            "Epoch 18, Losses: {'ner': 19.312953634012956}\n",
            "Epoch 19, Losses: {'ner': 17.25393970157893}\n",
            "Epoch 20, Losses: {'ner': 17.643029863440916}\n",
            "Epoch 21, Losses: {'ner': 17.246964328204456}\n",
            "Epoch 22, Losses: {'ner': 17.700293172580803}\n",
            "Epoch 23, Losses: {'ner': 15.509850187912727}\n",
            "Epoch 24, Losses: {'ner': 13.876898510381674}\n",
            "Epoch 25, Losses: {'ner': 14.998890688174304}\n",
            "Epoch 26, Losses: {'ner': 16.289265387450463}\n",
            "Epoch 27, Losses: {'ner': 11.640299108378883}\n",
            "Epoch 28, Losses: {'ner': 11.342513907514004}\n",
            "Epoch 29, Losses: {'ner': 10.518983957275404}\n",
            "Epoch 30, Losses: {'ner': 9.519692097459497}\n",
            "Epoch 31, Losses: {'ner': 10.964014378521066}\n",
            "Epoch 32, Losses: {'ner': 8.343609722632046}\n",
            "Epoch 33, Losses: {'ner': 6.615744473245876}\n",
            "Epoch 34, Losses: {'ner': 6.964798340072524}\n",
            "Epoch 35, Losses: {'ner': 4.798482513584283}\n",
            "Epoch 36, Losses: {'ner': 4.3437576856091376}\n",
            "Epoch 37, Losses: {'ner': 4.839764028537253}\n",
            "Epoch 38, Losses: {'ner': 6.302488946150953}\n",
            "Epoch 39, Losses: {'ner': 6.883787753977373}\n",
            "Epoch 40, Losses: {'ner': 4.1940355089339585}\n",
            "Epoch 41, Losses: {'ner': 3.767790200036013}\n",
            "Epoch 42, Losses: {'ner': 5.547814497282033}\n",
            "Epoch 43, Losses: {'ner': 1.9697961705621077}\n",
            "Epoch 44, Losses: {'ner': 4.318718927471121}\n",
            "Epoch 45, Losses: {'ner': 2.3990592088039158}\n",
            "Epoch 46, Losses: {'ner': 1.8158504071862973}\n",
            "Epoch 47, Losses: {'ner': 0.6340028714530006}\n",
            "Epoch 48, Losses: {'ner': 0.7799190858762776}\n",
            "Epoch 49, Losses: {'ner': 2.032800145347145}\n",
            "Epoch 50, Losses: {'ner': 0.9992933301009473}\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from spacy.util import minibatch\n",
        "import random\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "\n",
        "for label in classes:\n",
        "    ner.add_label(label)\n",
        "\n",
        "doc_bin = DocBin().from_disk(\"model/training_data.spacy\")\n",
        "docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "\n",
        "nlp.begin_training()\n",
        "\n",
        "for epoch in range(50):\n",
        "    losses = {}\n",
        "    random.shuffle(docs)\n",
        "    for batch in minibatch(docs, size=8):\n",
        "        for doc in batch:\n",
        "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "    print(f\"Epoch {epoch + 1}, Losses: {losses}\")\n",
        "\n",
        "nlp.to_disk(\"model/trained_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "0CS1c7knn5w6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QaIzayV3oxy4"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jUzlWoFr3Fgs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvJaTbrtjDJf",
        "outputId": "20fcb340-05d8-4a16-ef3c-866136d37385"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\vasan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoTw90De5MW2",
        "outputId": "8dc21cbf-8d4b-4139-f921-e6b6cf15b583"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\vasan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SLsEjZIZ5Iou"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jJuRPjkZpsO7"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  return np.dot(v1, v2) / (norm(v1) * norm(v2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kYS4s0IBoYLQ"
      },
      "outputs": [],
      "source": [
        "from text_corpus import TextCorpusSearcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CzhDNRTlsWQw"
      },
      "outputs": [],
      "source": [
        "place_search = TextCorpusSearcher(\n",
        "    \"data/train-place.txt\",\n",
        "    [\"chennai\", \"bangalore\"],\n",
        "    \"PLACE\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDVX1WeEqlqJ",
        "outputId": "5619443f-e0d1-45aa-a767-f2a6a4b3b6fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5104972, 'PLACE')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "place_search.get_score(\"jodhpur\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bJPTS2hE0Xk7"
      },
      "outputs": [],
      "source": [
        "service_search = TextCorpusSearcher(\n",
        "    \"data/train-service.txt\",\n",
        "    [\"cut\", \"shave\"],\n",
        "    \"SERVICE\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U74QFyZ-nOhF",
        "outputId": "c7208c9f-8b65-4bb4-ee84-48be3f4c4045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\vasan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sl2jNHv1rD8E"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7XGBc8WnivMV"
      },
      "outputs": [],
      "source": [
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "\n",
        "datasets = [\n",
        "    place_search,\n",
        "    service_search,\n",
        "]\n",
        "\n",
        "@Language.component(\"custom_ner\")\n",
        "def custom_ner_component(doc):\n",
        "    confidence = 0.5\n",
        "    new_entities = [ent for ent in doc.ents]\n",
        "    for index, token in enumerate(doc):\n",
        "        tag = pos_tag([token.text])[0][1]\n",
        "        if token.ent_type != 0 or tag != 'NN':\n",
        "          continue\n",
        "        maxLabel = \"\"\n",
        "        maxScore = 0\n",
        "        for searcher in datasets:\n",
        "          score, label = searcher.get_score(token.text)\n",
        "          if score > confidence and score > maxScore:\n",
        "            maxLabel = label\n",
        "            maxScore = score\n",
        "        if maxScore > 0:\n",
        "          print(\"Adding tag for \", token, maxLabel, \"with score\", maxScore)\n",
        "          new_entities.append(Span(doc, index, index + 1, label=maxLabel))\n",
        "    doc.ents = new_entities\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FaP0QVrehdcr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "v6WNom4gi6jS",
        "outputId": "f54d75f2-bb7c-486d-aca6-42f420fdfbd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.custom_ner_component(doc)>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.add_pipe(\"custom_ner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z8QquSvqZPE",
        "outputId": "4c20b25d-521b-457a-d140-c31879b17b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding tag for  Jodhpur PLACE with score 0.5104972\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"I want to go to Jodhpur and get a hair trim at 7pm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-DgyagKkFxe",
        "outputId": "a95dcb5b-371a-4a9d-f400-45ca3ccc31d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jodhpur\n"
          ]
        }
      ],
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HqUY8nkq1fxr"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.057224885, 'SERVICE')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "service_search.get_score(\"trim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Wna3wKVNhrSY",
        "outputId": "8a6ec46b-3078-4bef-8a85-4e9f0216d237"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I want to go to \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jodhpur\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
              "</mark>\n",
              " and get a hair trim at 7pm</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextCorpusSearcher:\n",
        "    def __init__(self, filename, x, label):\n",
        "        self.label = label\n",
        "        self.x = tuple(word.lower() for word in x)\n",
        "        text = self.get_text(filename)\n",
        "        sentences = []\n",
        "        for sent in sent_tokenize(text.lower()):\n",
        "            s = []\n",
        "            for word in word_tokenize(sent):\n",
        "                if word in stop_words or word in ',:;.':\n",
        "                    continue\n",
        "                s.append(word)\n",
        "            sentences.append(s)\n",
        "\n",
        "        self.model = Word2Vec(sentences, vector_size=24, window=3, min_count=1, sg=1)\n",
        "\n",
        "        for w in self.x:\n",
        "            if w not in self.model.wv:\n",
        "                print(\"[WARN]\", w, \"missing in Word2Vec training data\")\n",
        "\n",
        "    def get_text(self, filename):\n",
        "        with open(filename) as f:\n",
        "            return f.read() \n",
        "    \n",
        "    def get_embed(self, word):\n",
        "        return self.model.wv[word]\n",
        "\n",
        "    def get_score(self, word):\n",
        "        max_score = 0\n",
        "        global curr_model\n",
        "        word = word.lower()\n",
        "        if not pos_tag([word])[0][1].startswith('NN') or word not in self.model.wv:\n",
        "            return 0, self.label\n",
        "        curr_model = self.model\n",
        "        for w in self.x:\n",
        "            score = cosine_similarity(self.model.wv[word], self.model.wv[w])\n",
        "            max_score = max(max_score, score)\n",
        "        return max_score, self.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {},
      "outputs": [],
      "source": [
        "cities = ('Chail', 'Majuli', 'Malana', 'Mawlynnong', 'Gavi', 'Diskit', 'Landour', 'Idukki', 'Mandawa', 'Delhi', 'Mumbai', 'Bangalore', 'Kolkata', 'Chennai', 'Hyderabad', 'Jaipur', 'Pune', 'Ahmedabad', 'Varanasi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "execution_count": 505,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(cities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_labels = random.choices(cities, k=12)\n",
        "expected = [city for city in cities if city not in input_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Chail',\n",
              " 'Malana',\n",
              " 'Mawlynnong',\n",
              " 'Gavi',\n",
              " 'Diskit',\n",
              " 'Delhi',\n",
              " 'Kolkata',\n",
              " 'Hyderabad',\n",
              " 'Pune',\n",
              " 'Ahmedabad']"
            ]
          },
          "execution_count": 508,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = open(\"data/train-cities.txt\")\n",
        "tokens = word_tokenize(t.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [],
      "source": [
        "garbage = []\n",
        "while len(garbage) < len(expected):\n",
        "    w = random.choice(tokens)\n",
        "    if w in stop_words or w in ',.:;':\n",
        "        continue\n",
        "    garbage.append(w)\n",
        "t.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = [\n",
        "    (word, 1) for word in expected \n",
        "]\n",
        "\n",
        "test_data.extend(\n",
        "    [\n",
        "        (word, 0) for word in garbage\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = TextCorpusSearcher(filename=\"data/train-cities.txt\", x=input_labels, label=\"PLACE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test = []\n",
        "y_pred = []\n",
        "\n",
        "for word, is_expected in test_data:\n",
        "    y_test.append(is_expected)\n",
        "    y_pred.append(1 if ner.get_score(word.lower())[0] > 0.3 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]"
            ]
          },
          "execution_count": 515,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        10\n",
            "           1       0.89      0.80      0.84        10\n",
            "\n",
            "    accuracy                           0.85        20\n",
            "   macro avg       0.85      0.85      0.85        20\n",
            "weighted avg       0.85      0.85      0.85        20\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "execution_count": 518,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 553,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "\n",
        "datasets = [\n",
        "  ner,\n",
        "]\n",
        "\n",
        "@Language.component(\"custom_ner\")\n",
        "def custom_ner_component(doc):\n",
        "    confidence = 0.3\n",
        "    new_entities = [ent for ent in doc.ents]\n",
        "    for index, token in enumerate(doc):\n",
        "        tag = pos_tag([token.text])[0][1]\n",
        "        if token.ent_type != 0 or tag != 'NN':\n",
        "          continue\n",
        "        maxLabel = \"\"\n",
        "        maxScore = 0\n",
        "        for searcher in datasets:\n",
        "          score, label = searcher.get_score(token.text)\n",
        "          if score > confidence and score > maxScore:\n",
        "            maxLabel = label\n",
        "            maxScore = score\n",
        "        if maxScore > 0:\n",
        "          print(\"Adding tag for \", token, maxLabel, \"with score\", maxScore)\n",
        "          new_entities.append(Span(doc, index, index + 1, label=maxLabel))\n",
        "    doc.ents = new_entities\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 554,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.custom_ner_component(doc)>"
            ]
          },
          "execution_count": 554,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"custom_ner\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 560,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding tag for  Mawlynnong PLACE with score 0.33339307\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"I want to visit Mawlynnong sometime\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 561,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I want to visit \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mawlynnong\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
              "</mark>\n",
              " sometime</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
